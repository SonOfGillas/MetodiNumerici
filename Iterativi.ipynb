{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9c8bd-7f4e-4c62-a740-f261d308927b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dati dalla prof\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import scipy.linalg as spl\n",
    "import RisolviSis as RS\n",
    "import matplotlib.pyplot as plt\n",
    "dati = loadmat('testE.mat')\n",
    "A = dati[\"A\"]\n",
    "b = dati[\"b\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef242ee",
   "metadata": {},
   "source": [
    "### Jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe69b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definito anche metodo di decomposizione in cui la matrice di partenza viene\n",
    "# decomposta in 3 matrici che sono poi relazionate diversamente fra loro: A = D + E + F.\n",
    "# Per Jacobi vale la relazione: M = D e N = -(E + F).\n",
    "# L’algoritmo di Jacobi è definito se gli elementi diagonali di A sono diversi da 0 (a meno\n",
    "# che A sia non singolare e quindi riordinabile affiché il metodo sia applicabile) e\n",
    "# ogni elemento dell’iterato è indipendente dagli altri, pertanto il metodo\n",
    "# e ' parallelizzabile.\n",
    "# Il metodo non restituisce una soluzione ottima ma converge ad una soluzione definibile\n",
    "# ottima (approssimata).\n",
    "def jacobi(A, b, x0, toll, itmax):\n",
    "    n = A.shape[0]\n",
    "    # Estraggo la diagonale dalla matrice A, costruisco la matrice diagonale\n",
    "    d = np.diag(A)\n",
    "    D = np.diag(d)\n",
    "    # Creo la matrice triangolare inferiore da A, -1 perchè escludo la diagonale\n",
    "    E = np.tril(A, - 1)\n",
    "    # Estraggo dalla matrice A la sua matrice triangolare superiore,\n",
    "    # con diagonale esclusa\n",
    "    F = np.triu(A, 1)\n",
    "\n",
    "    # Decomposizione adottata nel metodo di Jacobi: si tiene a mente che M = D\n",
    "    N = -(E + F)\n",
    "\n",
    "    # Controllo il raggio spettrale che rispetti le regole di ammissibilità della soluzione.\n",
    "    # Si utilizza la condizione sufficiente come valida condizione di ricerca di una soluzione.\n",
    "    # Cercando il raggio spettrale, devo vedere che questo sia minore di 1\n",
    "    # affinché ci sia una veloce convergenza del metodo iterativo.\n",
    "    invM = np.diag(1 / d)\n",
    "    T = np.dot(invM, N)\n",
    "\n",
    "    # Il raggio spettrale di una matrice è il suo autovalore di modulo massimo\n",
    "    autT = np.linalg.eigvals(T)\n",
    "    rho = np.max(np.abs(autT))\n",
    "    print(\"Raggio spettrale: \", rho)\n",
    "    if (rho > 1):\n",
    "        print(\"Raggio spettrale maggiore di 1, nessuna soluzione\")\n",
    "\n",
    "    # Cuore dell'algoritmo: inizializzo il contatore delle iterazioni.\n",
    "    # Tengo traccia del vettore degli errori per poi fare un grafico\n",
    "    # rappresentativo.\n",
    "    it = 0\n",
    "    err_vet = []\n",
    "    err = 1000\n",
    "\n",
    "    while it <= itmax and err >= toll:\n",
    "        x = (b + np.dot(N, x0)) / d.reshape(n, 1)\n",
    "        # Se c'è convergenza le soluzioni non cambiano più\n",
    "        err = np.linalg.norm(x - x0) / np.linalg.norm(x)\n",
    "        err_vet.append(err)\n",
    "        # Al passo successivo, x0 sarà la x del passo precedente\n",
    "        x0 = x.copy()\n",
    "        it += 1\n",
    "\n",
    "    return x, it, err_vet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = A.shape[0]\n",
    "\n",
    "# Posso scegliere un qualunque vettore iniziale\n",
    "x0 = np.zeros((n, 1))\n",
    "# la tolleranza e il numero max di iterazioni li impostiamo perchè il\n",
    "# metodo converge ad una soluzione, non trova la soluzione al sistema\n",
    "# lineare. Solitamente si basa su una percentuale di errore da cui\n",
    "# sono affetti i dati.\n",
    "itmax = 100\n",
    "toll = 1e-8\n",
    "\n",
    "xJ, itJ, err_vetJ = jacobi(A, b, x0, toll, itmax)\n",
    "print(\"Soluzione:   \", xJ)\n",
    "print(\"Iterazioni:  \", itJ)\n",
    "\n",
    "plt.semilogy(np.arange(itJ), err_vetJ)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cf7a8ec",
   "metadata": {},
   "source": [
    "### Gauss-Siedel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df374ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definito anche metodo di decomposizione in cui la matrice di partenza viene\n",
    "# decomposta in 3 matrici che sono poi relazionate diversamente fra loro: A = D + E + F.\n",
    "# Per Gauss-Siedel vale la relazione: M = E + D e N = -F.\n",
    "# Gauss-Siedel converge sicuramente se la matrice e' simmetrica e definita positiva.\n",
    "# Questo metodo utilizza per calcolare la nuova componente di una iterazione tutte quelle\n",
    "# calcolate fino a quel punto: come conseguenza di ciò l'algoritmo non è parallelizzabile.\n",
    "# Il metodo non restituisce una soluzione ottima ma converge ad una soluzione definibile\n",
    "# ottima (approssimata).\n",
    "def gauss_seidel(A, b, x0, toll, itmax):\n",
    "    d = np.diag(A)\n",
    "    # Estraggo la diagonale dalla matrice A, costruisco la matrice diagonale\n",
    "    D = np.diag(d)\n",
    "    # Creo la matrice triangolare inferiore da A, -1 perchè escludo la diagonale\n",
    "    E = np.tril(A, - 1)\n",
    "    # Estraggo dalla matrice A la sua matrice triangolare superiore,\n",
    "    # con diagonale esclusa\n",
    "    F = np.triu(A, 1)\n",
    "\n",
    "    # Decomposizione adottata nel metodo di Gauss-Siedel:\n",
    "    M = D + E\n",
    "    N = -F\n",
    "\n",
    "    # Condizione necessaria e sufficiente alla convergenza.\n",
    "    # Il raggio spettrale di una matrice è il suo autovalore di modulo massimo\n",
    "    invM = np.linalg.inv(M)\n",
    "    T = np.dot(invM, N)\n",
    "\n",
    "    # Controllo il raggio spettrale che rispetti le regole di ammissibilità della soluzione.\n",
    "    # Si utilizza la condizione sufficiente come valida condizione di ricerca di una soluzione.\n",
    "    autT = np.linalg.eigvals(T)\n",
    "    rho = np.max(np.abs(autT))\n",
    "    print(\"Raggio spettrale: \", rho)\n",
    "    if (rho > 1):\n",
    "        print(\"Raggio spettrale maggiore di 1, nessuna soluzione\")\n",
    "\n",
    "    # Cuore dell'algoritmo: inizializzo il contatore delle iterazioni.\n",
    "    # Tengo traccia del vettore degli errori per poi fare un grafico\n",
    "    # rappresentativo.\n",
    "    it = 0\n",
    "    err_vet = []\n",
    "    err = 1000\n",
    "\n",
    "    while it <= itmax and err >= toll:\n",
    "        temp = b - np.dot(F, x0)\n",
    "        x, flag = RS.Lsolve(M, temp)\n",
    "        err = np.linalg.norm(x - x0) / np.linalg.norm(x)\n",
    "        err_vet.append(err)\n",
    "        x0 = x.copy()\n",
    "        it += 1\n",
    "\n",
    "    return x, it, err_vet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posso scegliere un qualunque vettore iniziale\n",
    "n = A.shape[0]\n",
    "x0 = np.zeros((n, 1))\n",
    "# la tolleranza e il numero max di iterazioni li impostiamo perchè il\n",
    "# metodo converge ad una soluzione, non trova la soluzione al sistema\n",
    "# lineare. Solitamente si basa su una percentuale di errore da cui\n",
    "# sono affetti i dati.\n",
    "itmax = 100\n",
    "toll = 1e-8\n",
    "\n",
    "xG, itG, err_vetG = gauss_seidel(A, b, x0, toll, itmax)\n",
    "print(\"Soluzione GS:    \", xG)\n",
    "print(\"Iterazioni GS:   \", itG)\n",
    "\n",
    "plt.semilogy(np.arange(itG), err_vetG)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d9d4dc",
   "metadata": {},
   "source": [
    "### Gauss-Siedel SOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definito anche metodo di decomposizione in cui la matrice di partenza viene\n",
    "# decomposta in 3 matrici che sono poi relazionate diversamente fra loro: A = D + E + F.\n",
    "# Per Gauss-Siedel SOR vale la relazione: M = E + D e N = -F.\n",
    "# Gauss-Siedel SOR converge sicuramente se la matrice e' simmetrica e definita positiva.\n",
    "# Questo metodo utilizza per calcolare la nuova componente di una iterazione tutte quelle\n",
    "# calcolate fino a quel punto: come conseguenza di ciò l'algoritmo non è parallelizzabile.\n",
    "# Il metodo non restituisce una soluzione ottima ma converge ad una soluzione definibile\n",
    "# ottima (approssimata).\n",
    "# A differenza di Gauss-Siedel classico, il metodo subisce una accelerazione verso la\n",
    "# soluzione ottima sfruttando un parametro omega di rilassamento.\n",
    "def gauss_seidel_sor(A, b, x0, omega, toll, itmax):\n",
    "    errore = 1000\n",
    "    d = np.diag(A)\n",
    "    D = np.diag(d)\n",
    "    Dinv = np.diag(1/d)\n",
    "    # Estraggo la diagonale dalla matrice A, costruisco la matrice diagonale.\n",
    "    E = np.tril(A, -1)\n",
    "    # Creo la matrice triangolare inferiore da A, -1 perchè escludo la diagonale.\n",
    "    F = np.triu(A, 1)\n",
    "\n",
    "    # Decomposizione adottata nel metodo di Gauss-Siedel SOR: devo introdurre\n",
    "    # un parametro omega che riduca il più possibile il raggio spettrale.\n",
    "    # Questo si fa' perché il problema principale della convergenza del metodo\n",
    "    # e' legato al mal condizionamento di A, che causa il rallentamento oppure\n",
    "    # la perdita della convergenza stessa del metodo.\n",
    "    M_omega = D + omega * E\n",
    "    N_omega = (1 - omega) * D - omega * F\n",
    "    T = np.dot(np.linalg.inv(M_omega), N_omega)\n",
    "    M = D + E\n",
    "    N = -F\n",
    "\n",
    "    # Il raggio spettrale di una matrice è il suo autovalore di modulo massimo.\n",
    "    autovalori = np.linalg.eigvals(T)\n",
    "    raggiospettrale = np.max(np.abs(autovalori))\n",
    "    print(\"raggio spettrale Gauss-Seidel SOR \", raggiospettrale)\n",
    "\n",
    "    # Cuore dell'algoritmo: inizializzo il contatore delle iterazioni.\n",
    "    # Tengo traccia del vettore degli errori per poi fare un grafico\n",
    "    # rappresentativo.\n",
    "    it = 0\n",
    "    x_old = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    er_vet = []\n",
    "\n",
    "    while it <= itmax and errore >= toll:\n",
    "        temp = b - np.dot(F, x_old)\n",
    "        x_tilde, flag = RS.Lsolve(M, temp)\n",
    "        # Il parametro omega agisce qui per accelerare la convergenza\n",
    "        x_new = (1 - omega) * x_old + omega * x_tilde\n",
    "        errore = np.linalg.norm(x_new - x_old) / np.linalg.norm(x_new)\n",
    "        er_vet.append(errore)\n",
    "        x_old = x_new.copy()\n",
    "        it += 1\n",
    "    return x_new, it, er_vet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posso scegliere un qualunque vettore iniziale\n",
    "n = A.shape[0]\n",
    "x0 = np.zeros((n, 1))\n",
    "\n",
    "# se 0 < omega < 1, il metodo di GS non ha convergenza\n",
    "# se omega > 1, il metodo di GS converge lentamente\n",
    "omega = 1.4\n",
    "# la tolleranza e il numero max di iterazioni li impostiamo perchè il\n",
    "# metodo converge ad una soluzione, non trova la soluzione al sistema\n",
    "# lineare. Solitamente si basa su una percentuale di errore da cui\n",
    "# sono affetti i dati.\n",
    "itmax = 100\n",
    "toll = 1e-8\n",
    "\n",
    "xG, itG, err_vetG = gauss_seidel_sor(A, b, x0, omega, toll, itmax)\n",
    "print(\"Soluzione GSS:    \", xG)\n",
    "print(\"Iterazioni GSS:   \", itG)\n",
    "\n",
    "plt.semilogy(np.arange(itG), err_vetG)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a540fef",
   "metadata": {},
   "source": [
    "### Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepestdescent(A, b, x0, itmax, toll):\n",
    "    # Definisco l'approssimazione iniziale della soluzione.\n",
    "    x = x0\n",
    "\n",
    "    # Calcolo il residuo ...\n",
    "    r = A.dot(x) - b\n",
    "\n",
    "    # ... che poi imposto come direzione di discesa.\n",
    "    p = -r\n",
    "\n",
    "    # Definisco l'errore di partenza e altre cose che\n",
    "    # mi serviranno per dare il via al cuore dell'\n",
    "    # algoritmo.\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    errore = np.linalg.norm(r) / norm_b\n",
    "    vec_sol = []\n",
    "    vec_sol.append(x)\n",
    "    vet_residuo = []\n",
    "    vet_residuo.append(errore)\n",
    "\n",
    "    # Il metodo della discesa ripida ha come caratteristica\n",
    "    # particolare di scegliere la direzione p k-esima come\n",
    "    # l’antigradiente della F calcolato nell’iterato\n",
    "    # k-esimo, che in questo caso è letta come la direzione\n",
    "    # di massima decrescita.\n",
    "    it = 0\n",
    "    while errore >= toll and it < itmax:\n",
    "        it += 1\n",
    "\n",
    "        # Scelta dello step size: per ottenere il minimo\n",
    "        # valore possibile del gradiente lungo la\n",
    "        # direzione scelta.\n",
    "        A_p = A.dot(p)\n",
    "        rTr = np.dot(r.T, r)\n",
    "        alpha = rTr / np.dot(p.T, A_p)\n",
    "\n",
    "        # Aggiorno l'iterato.\n",
    "        x = x + alpha * p\n",
    "        r = r + alpha * A_p\n",
    "\n",
    "        # Salvo i dati per l'output.\n",
    "        vec_sol.append(x)\n",
    "        errore = np.linalg.norm(r) / norm_b\n",
    "        vet_residuo.append(errore)\n",
    "\n",
    "        # \"Discendo\".\n",
    "        p = -r\n",
    "\n",
    "    return x, vet_residuo, vec_sol, it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indice di condizionamento della matrice A: indica\n",
    "# quanto e' lenta la convergenza.\n",
    "print(\"Condizionameto di A\", np.linalg.cond(A))\n",
    "\n",
    "toll = 1e-8\n",
    "it_max = 10000\n",
    "x0 = np.zeros_like(b)\n",
    "\n",
    "# Dal grafico si nota carattere a zigzag del metodo\n",
    "# del gradiente, dovuto al fatto che il gradiente di\n",
    "# una iterata è ortogonale al gradiente di quello\n",
    "# precedente. Nonostante la convergenza dell'algoritmo,\n",
    "# questo è relativamente lento a causa di questo\n",
    "# avanzamento a zig zag.\n",
    "x_gr, vet_r_gr, vec_sol_gr, itG = steepestdescent(A, b, x0, it_max, toll)\n",
    "print(\"Iterazioni Gradiente \", itG)\n",
    "\n",
    "plt.semilogy(np.arange(itG + 1), vet_r_gr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cca186a",
   "metadata": {},
   "source": [
    "### Gradiente Coniugato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f89990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0, itmax, toll):\n",
    "    # Definisco l'approssimazione iniziale della soluzione.\n",
    "    x = x0\n",
    "\n",
    "    # Calcolo del residuo ...\n",
    "    r = A.dot(x) - b\n",
    "\n",
    "    # ... che poi imposto come direzione di discesa.\n",
    "    p = -r\n",
    "\n",
    "    # Definisco l'errore di partenza e altre cose che\n",
    "    # mi serviranno per dare il via al cuore dell'\n",
    "    # algoritmo.\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    errore = np.linalg.norm(r) / norm_b\n",
    "    vec_sol = []\n",
    "    vec_sol.append(x)\n",
    "    vet_residuo = []\n",
    "    vet_residuo.append(errore)\n",
    "\n",
    "    # Il metodo del gradiente coniugato, a differenza di\n",
    "    # quello del gradiente a discesa ripida, non tiene\n",
    "    # conto solo della direzione del gradiente ma anche\n",
    "    # di quella che era la direzione scelta nell'iterata\n",
    "    # precedente.\n",
    "    # Il numero di iterazioni che occorrono per\n",
    "    # raggiungere la precisione richiesta e' di gran\n",
    "    # lunga inferiore alla dimensione del sistema e\n",
    "    # questo rende il metodo molto utile per problemi\n",
    "    # di grosse dimensioni.\n",
    "    it = 0\n",
    "    while errore >= toll and it < itmax:\n",
    "        it += 1\n",
    "\n",
    "        # Scelta dello step size: per ottenere il minimo\n",
    "        # valore possibile del gradiente lungo la\n",
    "        # direzione scelta.\n",
    "        A_p = A.dot(p)\n",
    "        rtr = np.dot(r.T, r)\n",
    "        alpha = rtr / np.dot(p.T, A_p)\n",
    "\n",
    "        # Aggiorno l'iterato tenendo conto sia della\n",
    "        # direzione del gradiente e sia della iterazione\n",
    "        # precedente.\n",
    "        x = x + alpha * p\n",
    "        r = r + alpha * A_p\n",
    "        # direzione scelta in modo che punti verso il\n",
    "        # centro della ellissi di convergenza.\n",
    "        gamma = np.dot(r.T, r) / rtr\n",
    "\n",
    "        # Salvo i dati per l'output.\n",
    "        vec_sol.append(x)\n",
    "        errore = np.linalg.norm(r) / norm_b\n",
    "        vet_residuo.append(errore)\n",
    "\n",
    "        # \"Discendo\".\n",
    "        p = -r + gamma * p\n",
    "\n",
    "    return x, vet_residuo, vec_sol, it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indice di condizionamento della matrice A: a differenza\n",
    "# del metodo dello steepest descent, a parita di indice\n",
    "# di condizionamento questo risulta più veloce.\n",
    "print(\"Condizionameto di A\", np.linalg.cond(A))\n",
    "\n",
    "toll = 1e-8\n",
    "it_max = 10000\n",
    "x0 = np.zeros_like(b)\n",
    "\n",
    "# Dal grafico si nota carattere a zigzag del metodo\n",
    "# del gradiente coniugato, seppur questo sia molto più\n",
    "# veloce rispetto a quello dello steepest descent.\n",
    "x_cg, vet_r_cg, vec_sol_cg, itCG = conjugate_gradient(A, b, x0, it_max, toll)\n",
    "print(\"Iterazioni Gradiente Coniugato \", itCG)\n",
    "\n",
    "plt.semilogy(np.arange(itCG + 1), vet_r_cg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
